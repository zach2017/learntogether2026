# --- Model ---
base_model: Qwen/Qwen2.5-Coder-1.5B-Instruct     # or Qwen/Qwen2.5-Coder-0.5B-Instruct
datasets:
  - path: /workspace/data/train.jsonl            # path INSIDE the container
    type: chat_template
    field_messages: messages
    roles_to_train: ["assistant"]

# --- Splits ---
val_set_size: 0.05

# --- Context & packing ---
sequence_len: 4096
sample_packing: true

# --- Precision & memory ---
bf16: true
load_in_4bit: true
adapter: qlora
gradient_checkpointing: true

# --- LoRA ---
lora_r: 16
lora_alpha: 32
lora_dropout: 0.0
lora_mlp_kernel: true
lora_qkv_kernel: true

# --- Optim & schedule ---
optimizer: adamw_bnb_8bit
learning_rate: 1.0e-4
lr_scheduler: cosine
warmup_ratio: 0.05

# --- Batching (safe defaults) ---
micro_batch_size: 1
gradient_accumulation_steps: 16
num_epochs: 1

# --- Logging / saving ---
output_dir: /workspace/outputs/qwen25-coder-qlora
save_steps: 100
eval_steps: 100
logging_steps: 10
save_safetensors: true
